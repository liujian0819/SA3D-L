
SA3D-L: A Light-weight Model for 3D Object Segmentation Using
Neural Radiance Fields

![image](https://github.com/liujian0819/SA3D-L/blob/main/Video1_64913419.gif)
<!DOCTYPE html>
<html>
<head> 
</head>
<body>  
<bold>Abstract: Segmentation in three dimensions is a major hurdle in computer vision. The Segment Anything
Model (SAM) has recently achieved considerable advances in the segmentation of objects within 2D
images. Neural radiance fields (NeRFs) efficiently utilize a multilayer perceptron (MLP) to learn the
continuous representation of a 3D scene. Due to the consistency of 3D perspectives from multiple
viewpoints, SAM, which was initially designed for 2D image segmentation, can be adapted for
effective 3D object segmentation. However, one drawback is that MLP provides a representation of
the entire scene without distinguishing separate objects. In this study, we introduced a novel model
called SA3D-L, which modifies the MLP output in the NeRFs to individually represent each object in
the scene. The experimental results on the established benchmarks showed that the 3D segmentation
representations of each object can be derived from its 2D masks. Therefore, this approach allows for
the independent manipulation of these objects to reconstruct new scenes.
</body>
</html>
